from math import sqrt
from numpy import concatenate
from matplotlib import pyplot
import numpy as np
from pandas import read_csv
from pandas import DataFrame
from keras.models import load_model
from pandas import concat
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import mean_squared_error
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import LSTM
from keras.layers.core import RepeatVector
from keras.layers import Dropout, Activation, Flatten
from keras.layers import Convolution2D, MaxPooling2D
from keras.optimizers import SGD
from keras.models import model_from_yaml
import os
from keras.utils import plot_model
from keras.utils.vis_utils import plot_model
from keras.utils.vis_utils import model_to_dot
def gothrougheveryfile(dir):  # go through every csv and concat to one csv this part has move to concatallcsv
    i = 0
    filenamelist = list()
    for filename in os.listdir(dir):  # outside each folder
        wholefilepath = dir + "//" + filename
        filenamelist.append(wholefilepath)
    return filenamelist

def data_to_reconstruction_problem(data, timestep):
    df = DataFrame(data)
    list_concat = list()
    for i in range(timestep - 1, -1, -1):
        tempdf = df.shift(i)
        list_concat.append(tempdf)
    data_for_autoencoder = concat(list_concat, axis=1)
    data_for_autoencoder.dropna(inplace=True)
    return data_for_autoencoder


def out_put_core(writting_list, outaddress, filename):
    thefile = open(outaddress + "\\" + filename + ".txt", 'w')
    for item in writting_list:
        thefile.write("%s\n" % item)

def data_preprocess(file_name, timestep):
    # read
    dataset = read_csv(file_name, header=None, index_col=None)
    if dataset.shape[0]<16:
        return 0,0
    values = dataset.values
    reframed = data_to_reconstruction_problem(values, timestep)
    reframedvalues = reframed
    reframed = reframed.astype('float32')
    scaler = MinMaxScaler(feature_range=(0, 1))
    scaled = scaler.fit_transform(reframed)
    dfscaled = DataFrame(scaled)
    valuescaled =dfscaled.values
    return  valuescaled,scaler,reframedvalues


def SingleFileLstmAutoencoder(apa, batch, n_apis, n_features, data_for_model_training):
    #error_list = list()
    W_Hidden1_list = list()
    W_Hidden2_list = list()
    W_Hidden3_list = list()
    W_Hidden4_list = list()
    W_Hidden5_list = list()
    train_X, scaler, y = data_preprocess(data_for_model_training, n_apis)
    train_X = train_X.reshape((train_X.shape[0], n_apis, n_features))
    sample_number = train_X.shape[0]
    outputlayer2 = n_apis #16
    outputlayer3 = int(n_apis / 2) #8
    timesstep4 = int(n_apis / 4) # 4
    model = Sequential()
    model.add(LSTM(n_features, input_shape=(n_apis, train_X.shape[2]), return_sequences=True))  # train_X.shape[2] = 34
    model.add(LSTM(outputlayer2, return_sequences=True))
    model.add(LSTM(outputlayer3, return_sequences=True))
    model.add(LSTM(outputlayer2, return_sequences=True))
    model.add(LSTM(n_features, return_sequences=True))
    model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])
    history = model.fit(train_X, train_X, epochs=apa, batch_size=batch, shuffle=False)  # train
    return model
    #model.save(saved_model)

def train5modelAE(apa, batch, timestep, numfeature, filea, fileb, filec, filed, filee):
    #SingleFileLstmAutoencoder(apa, batch, n_apis, n_features, data_for_model_training)
    k1 = SingleFileLstmAutoencoder(apa, batch, timestep, numfeature, filea)
    k2 = SingleFileLstmAutoencoder(apa, batch, timestep, numfeature, fileb)
    k3 = SingleFileLstmAutoencoder(apa, batch, timestep, numfeature, filec)
    k4 = SingleFileLstmAutoencoder(apa, batch, timestep, numfeature, filed)
    k5 = SingleFileLstmAutoencoder(apa, batch, timestep, numfeature, filee)
    return k1, k2, k3, k4, k5


apa = 200
batch = 10
timestep = 16
numfeature = 25
data_for_model_training_1 = r"C:\Users\pups1\PycharmProjects\project\Lstmautoencoder_for_malwareapi\finalcsv\Browsefox\1ee514676a8305eebf1b945d0c14e94b17e2b950ee2b2cad054623675a033199_3256.csv"
data_for_model_training_2 = r"C:\Users\pups1\PycharmProjects\project\Lstmautoencoder_for_malwareapi\finalcsv\Browsefox\2bf54e49fc3e09e889819a47d68cf82c975542d3f0a6a41e6306fdb25d74a0c5_3316.csv"
data_for_model_training_3 = r"C:\Users\pups1\PycharmProjects\project\Lstmautoencoder_for_malwareapi\finalcsv\Browsefox\2ccbcc893d4baa3f44afadb97e097a14037e83e0972eed309ce5eb3490f6f78b_3324.csv"
data_for_model_training_4 = r"C:\Users\pups1\PycharmProjects\project\Lstmautoencoder_for_malwareapi\finalcsv\Browsefox\3aa60ef86be648b44c67d4eb61ba978d06b554f82df1c1e65bc039bfb334e657_3292.csv"
data_for_model_training_5 = r"C:\Users\pups1\PycharmProjects\project\Lstmautoencoder_for_malwareapi\finalcsv\Browsefox\2160d853ced0f5d19c35fb0e655e9cdcc9811b7a3259380a8404edeb13a216db_3244.csv"
k1, k2, k3, k4, k5 = train5modelAE(apa, batch, timestep, numfeature, data_for_model_training_1, data_for_model_training_2, data_for_model_training_3, data_for_model_training_4, data_for_model_training_5)
filename =  r"/Users/yanyaosheng/Desktop/work_keras/final_csv/Browsefox/43cf10fbebf225aafd0bb7670bcf538d425e7276f095b667a8e5094ee0a6d2ef_3276.csv"
timestep = 16
numfeature = 25
apa = 10
batch = 4

def postboosting(filename, timestep, numfeature, apa, batch):
    test_x, scaler,y = data_preprocess(filename, timestep)
    test_x = test_x.reshape(test_x.shape[0], timestep, numfeature)
    yhatk1 = k1.predict(test_x)
    yhatk2 = k2.predict(test_x)
    yhatk3 = k3.predict(test_x)
    yhatk4 = k4.predict(test_x)
    yhatk5 = k5.predict(test_x)
    inputofdense = np.concatenate((yhatk1, yhatk2, yhatk3, yhatk4, yhatk5), 1).reshape(test_x.shape[0], 5, numfeature*timestep,1)
    test_x = test_x.reshape(test_x.shape[0], 1, numfeature*timestep, 1)
    modelmerge = Sequential()
    modelmerge.add(Convolution2D(nb_filter = 1,nb_row = 5,nb_col =  1, border_mode='valid', input_shape=(5,numfeature*timestep,1)))
    modelmerge.compile(loss='mean_squared_error', metrics=['accuracy'], optimizer='adam')
    history = modelmerge.fit(inputofdense, test_x,   nb_epoch=apa,  batch_size=batch)

    return modelmerge
modelmerge =postboosting(filename, timestep, numfeature, apa, batch)


filepathlist = r"/Users/yanyaosheng/Desktop/work_keras/final_csv/Browsefox/43cf10fbebf225aafd0bb7670bcf538d425e7276f095b667a8e5094ee0a6d2ef_3276.csv"

test_x, scaler,y = data_preprocess(filepathlist, 16)
test_x = test_x.reshape(test_x.shape[0], 16, 25)

#test_x = test_x.reshape(146, 400)

yhatk1 = k1.predict(test_x)
yhatk2 = k2.predict(test_x)
yhatk3 = k3.predict(test_x)
yhatk4 = k4.predict(test_x)
yhatk5 = k5.predict(test_x)

inputofdense = np.concatenate((yhatk1, yhatk2, yhatk3, yhatk4, yhatk5), 1).reshape(146, 5, 400,1)


yhat = modelmerge.predict(inputofdense)
yhat = yhat.reshape(yhat.shape[0], 16 * 25)
yhat = scaler.inverse_transform(yhat)
print("===")
print(yhat.shape)
print(y.shape)
rmse = np.sqrt(np.mean(((yhat - y) ** 2),axis=1))
print(np.mean(rmse))
print(modelmerge.layers[0].get_weights()[0])