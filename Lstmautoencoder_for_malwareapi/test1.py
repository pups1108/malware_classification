from math import sqrt
from numpy import concatenate
from matplotlib import pyplot
import numpy as np
from pandas import read_csv
from pandas import DataFrame
from pandas import concat
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import mean_squared_error
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import LSTM
from keras.layers.core import RepeatVector
from keras.models import model_from_yaml
import os
from keras.utils import plot_model

from keras.utils.vis_utils import plot_model

from keras.utils.vis_utils import model_to_dot

# 多階層的lstm的autoencoder
# 用來壓縮hooklog 輸出hooklog hat然後跟原始的hooklog比對
# 取latent做分類用
# 此一檔案test1
# 用來一筆資料建model之後
# 同一個家族的其他筆csv拿來測準確度
# stride is 1 所以資料sampleg數會少15筆
folder_address = "K:\\aaa"
preoutaddress = "C:\\Users\\Colour\\Desktop\\weight\\"
malware_class_dir = "C:\\Users\\pups1\\PycharmProjects\\project\\Lstmautoencoder_for_malwareapi\\almanahe"
apa = 1000
batch = 10


def gothrougheveryfile(dir):  # go through every csv and concat to one csv this part has move to concatallcsv
    i = 0
    filenamelist = list()
    for filename in os.listdir(dir):  # outside each folder

        wholefilepath = dir + "//" + filename
        filenamelist.append(wholefilepath)

    return filenamelist


def data_to_reconstruction_problem(data, timestep):
    df = DataFrame(data)
    list_concat = list()
    for i in range(timestep - 1, -1, -1):
        tempdf = df.shift(i)
        list_concat.append(tempdf)
    data_for_autoencoder = concat(list_concat, axis=1)
    data_for_autoencoder.dropna(inplace=True)
    return data_for_autoencoder


def data_to_reconstruction_problem(data, timestep):
    df = DataFrame(data)
    list_concat = list()
    for i in range(timestep - 1, -1, -1):
        tempdf = df.shift(i)
        list_concat.append(tempdf)
    data_for_autoencoder = concat(list_concat, axis=1)
    data_for_autoencoder.dropna(inplace=True)
    return data_for_autoencoder


def out_put_core(writting_list, outaddress, filename):
    thefile = open(outaddress + "\\" + filename + ".txt", 'w')
    for item in writting_list:
        thefile.write("%s\n" % item)

def data_preprocess(file_name):
    # read
    dataset = read_csv(file_name, header=None, index_col=None)
    if dataset.shape[0]<=16:
        return 0,0

    values = dataset.values

    reframed = data_to_reconstruction_problem(values, n_apis)


    print("reframed")
    print(reframed.shape)

    # to 0 - 1
    reframed = reframed.astype('float32')
    scaler = MinMaxScaler(feature_range=(0, 1))
    scaled = scaler.fit_transform(reframed)
    # print ("deal with: "+file_name+" in folder: "+folder+" shape: "+str(dfscaled.shape))
    dfscaled = DataFrame(scaled)
    valuescaled =dfscaled.values

    return  valuescaled,scaler

'''  
for folder in os.listdir(folder_address):
    if folder.endswith('.py') or folder.endswith('.txt'):
        continue
    if not os.path.exists(preoutaddress+folder):
        os.makedirs(preoutaddress+folder)
    file_address = folder_address + "\\" + folder
    outputaddress = preoutaddress + folder
    for file in os.listdir(file_address):
    '''
############################################################# tab 2

# file_name = file_address + "\\" + file
data_for_model_training = "e103de89e5320990d334bde4e8def5b294163f2c46702286812d7ece8ffdef9a_3216.csv"

filepathlist = gothrougheveryfile(malware_class_dir)
n_apis = 16
n_features = 34

train_X, scaler = data_preprocess(data_for_model_training)

# train_X.shape[0] is row number in csv(sample number)
train_X = train_X.reshape((train_X.shape[0], n_apis, n_features))
sample_number = train_X.shape[0]

timesstep16 = n_apis
timesstep8 = int(n_apis / 2)
timesstep4 = int(n_apis / 4)
# model.add(LSTM(34, input_shape=(train_X.shape[1], train_X.shape[2]), return_sequences=False))

# 拉model structure
# model layer
model = Sequential()
model.add(LSTM(n_features, input_shape=(n_apis, train_X.shape[2]), return_sequences=True))

model.add(LSTM(timesstep16, return_sequences=True))

model.add(LSTM(timesstep8, return_sequences=True))

model.add(LSTM(timesstep16, return_sequences=True))

model.add(LSTM(n_features, return_sequences=True))
model.compile(loss='mse', optimizer='adam')
# model layer end

# start training
history = model.fit(train_X, train_X, epochs=apa, batch_size=batch, shuffle=False)  # train


pyplot.plot(history.history['loss'], label='train')
pyplot.legend()
# pyplot.show()

#lossepoch = "{}.png".format(os.path.basename(filepathlist[i]))
lossepoch = "{}.png".format(data_for_model_training)
pyplot.savefig(lossepoch)
pyplot.gcf().clear()
for i in range(0, len(filepathlist)):
    print(filepathlist[i])

    test_x, scaler = data_preprocess(filepathlist[i])

    if test_x is 0:
        continue

    print(test_x.shape)
    test_x = test_x.reshape(test_x.shape[0], n_apis, n_features)
    yhat = model.predict(test_x)
    # 輸出的yhat.shape是 (sample數,apis,n_feature(34)) 其中sample數同原始sample數就是原始csv檔的個數
    # 因此要reshape成原本的 shape(transform之後的shape)才能 inverse_transform
    yhat = yhat.reshape(yhat.shape[0], n_apis * n_features)
    yhat = scaler.inverse_transform(yhat)
    dfyhat = DataFrame(yhat)
    yhat_reconstruction_value = "recons.{}.csv".format(os.path.basename(filepathlist[i]))
    dfyhat.to_csv(yhat_reconstruction_value)


    test_x =0
    scaler =0

