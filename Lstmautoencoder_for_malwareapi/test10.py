from math import sqrt
from numpy import concatenate
from matplotlib import pyplot
import numpy as np
from pandas import read_csv
from pandas import DataFrame
from pandas import concat
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import mean_squared_error
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import LSTM
from keras.layers.core import RepeatVector
from keras.models import model_from_yaml
from keras.models import load_model
import os
from keras.utils import plot_model

from keras.utils.vis_utils import plot_model

from keras.utils.vis_utils import model_to_dot

# 多階層的lstm的autoencoder
# 用來壓縮hooklog 輸出hooklog hat然後跟原始的hooklog比對
# 取latent做分類用
# 此一檔案test1
# 用來一筆資料建model之後
# 同一個家族的其他筆csv拿來測準確度
# stride is 1 所以資料sampleg數會少15筆
# test3.py用來測試儲存訓練好的model 先用almanahe class
# 寫出error和檔案的對應關係到 error_file_mapping
# 參數

# test4.py
# error_file_mapping 寫出error(rmse)和對應檔位置的檔案名
# saved_model Load進來的 model名稱 predict用
# test4.py 僅做predict的部分沒有training的

#test8.py
# 用combine的autoencoder機器來predict 某個類別實驗有沒有抓到該類別的精華


#malware_class_dir = "C:\\Users\\pups1\\PycharmProjects\\project\\Lstmautoencoder_for_malwareapi\\barys"
malware_class_dir = r"C:\Users\pups1\PycharmProjects\project\Lstmautoencoder_for_malwareapi\finalcsv\Bdmj"
error_file_mapping = "error_file_mapping.txt"
saved_model ="model_combine_autoencoder2.h5"
#saved_model ="model_apa1000.h5"
#saved_model = "apa1model.h5"



def gothrougheveryfile(dir):  # go through every csv and concat to one csv this part has move to concatallcsv
    i = 0
    filenamelist = list()
    for filename in os.listdir(dir):  # outside each folder

        wholefilepath = dir + "//" + filename
        filenamelist.append(wholefilepath)

    return filenamelist


def data_to_reconstruction_problem(data, timestep):
    df = DataFrame(data)
    list_concat = list()
    for i in range(timestep - 1, -1, -1):
        tempdf = df.shift(i)
        list_concat.append(tempdf)
    data_for_autoencoder = concat(list_concat, axis=1)
    data_for_autoencoder.dropna(inplace=True)
    return data_for_autoencoder


def data_to_reconstruction_problem(data, timestep):
    df = DataFrame(data)
    list_concat = list()
    for i in range(timestep - 1, -1, -1):
        tempdf = df.shift(i)
        list_concat.append(tempdf)
    data_for_autoencoder = concat(list_concat, axis=1)
    data_for_autoencoder.dropna(inplace=True)
    return data_for_autoencoder


def out_put_core(writting_list, outaddress, filename):
    thefile = open(outaddress + "\\" + filename + ".txt", 'w')
    for item in writting_list:
        thefile.write("%s\n" % item)

def data_preprocess(file_name):
    # read file
    dataset = read_csv(file_name, header=None, index_col=None)
    #如果csv sample數量少於16則會發生錯誤因此返回
    if dataset.shape[0]<=16:
        return 0,0,0

    values = dataset.values

    reframed = data_to_reconstruction_problem(values, n_apis)

    #return inorder to compare test_x y_hat
    reframedvalues =reframed


    print("reframed")
    print(reframed.shape)

    # to 0 - 1
    reframed = reframed.astype('float32')
    scaler = MinMaxScaler(feature_range=(0, 1))
    scaled = scaler.fit_transform(reframed)
    # print ("deal with: "+file_name+" in folder: "+folder+" shape: "+str(dfscaled.shape))
    dfscaled = DataFrame(scaled)
    valuescaled =dfscaled.values

    return  valuescaled,scaler,reframedvalues

n_apis = 16
n_features = 25
filepathlist = gothrougheveryfile(malware_class_dir)
error_list = list()
file_list = list()

for i in range(0,len(filepathlist)):
    print(filepathlist[i])

    #test_x has scaled
    test_x, scaler,y = data_preprocess(filepathlist[i])

    if test_x is 0:
        continue

    #print(test_x.shape)
    test_x = test_x.reshape(test_x.shape[0], n_apis, n_features)
    # returns a compiled model
    # identical to the previous one
    model = load_model(saved_model)

    #當我們在train的時候採用的是展開(timestep16)後的樣本，因此預測的自己也(yhat)會是展開後的樣本
    yhat = model.predict(test_x)
    # 輸出的yhat.shape是 (sample數,apis,n_feature(34)) 其中sample數同原始sample數就是原始csv檔的個數
    # 因此要reshape成原本的 shape(transform之後的shape)才能 inverse_transform
    yhat = yhat.reshape(yhat.shape[0], n_apis * n_features)
    yhat = scaler.inverse_transform(yhat)

    #compute error
    #mse = ((y - yhat) ** 2).mean(axis=1)
    # 平均是所有element都參與，如果要一row一row平均必須採用 axis
    rmse = np.sqrt(np.mean(((yhat - y) ** 2),axis=1))
    #print(mse)
    #print(mse.shape)
    print ((rmse))

    #因為前面是每個樣本都取一個平均所以這裡要把所有樣本的誤差再平均一次
    error_list.append(np.mean(rmse))
    #error_list.append(rmse / 16)

    file_list.append(filepathlist[i])



    dfyhat = DataFrame(yhat)
    yhat_reconstruction_value = "recons.{}.csv".format(os.path.basename(filepathlist[i]))
    dfyhat.to_csv(yhat_reconstruction_value)


    test_x =0
    scaler =0

'''
thefile = open('test.txt', 'w')
print ("ssssssssss")
print (len(error_list))
print ("ssssssssssa")
print (len(error_list))

for item in zip(file_list, error_list):
    thefile.write("%s\n" % item)
#thefile.write("%s\n" % item)

'''
print("len(error_list)")
print (len(error_list))

#open如果寫在for迴圈裡面會導致覆蓋，因此只會有一行
fo = open(error_file_mapping, "w")
for k in range(0,len(error_list)):
    print("hellohello")

    fo.write(str(file_list[k])+"\n")
    fo.write(str(error_list[k])+"\n")


fo.close()
