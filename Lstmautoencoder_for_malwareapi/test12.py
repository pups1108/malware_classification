from math import sqrt
from numpy import concatenate
from matplotlib import pyplot
import numpy as np
from pandas import read_csv
from pandas import DataFrame
from pandas import concat
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import mean_squared_error
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import LSTM
from keras.layers.core import RepeatVector
from keras.models import model_from_yaml
import os
from keras.utils import plot_model

from keras.utils.vis_utils import plot_model

from keras.utils.vis_utils import model_to_dot

#可決定的參數
# data_for_model_training 要用哪筆csv去做training
# saved_model 要儲存的model名稱
# apa 要訓練的次數
# batch

# 多階層的lstm的autoencoder
# 用來壓縮hooklog 輸出hooklog hat然後跟原始的hooklog比對
# 取latent做分類用
# 此一檔案test1
# 用來一筆資料建model之後
# 同一個家族的其他筆csv拿來測準確度
# stride is 1 所以資料sampleg數會少15筆
# test3.py用來測試儲存訓練好的model
# 可用這個檔案來產生model 並儲存
# test3.py 為training 的部分
# test6.py 不將 data展開成為每個row16個timestep的格式
# 嘗試重寫成 1個row一個timestep
# 將一個類別裡的每個autoencoder合併嘗試找到一個大的autoencoder來代表這個家族

#  test12 用於train browsefox單一sample的ae 用於當作基準點來比較


#malware_class_dir = "C:\\Users\\pups1\\PycharmProjects\\project\\Lstmautoencoder_for_malwareapi\\barys"
malware_class_dir = r"C:\Users\pups1\PycharmProjects\project\Lstmautoencoder_for_malwareapi\finalcsv\Browsefox"
data_for_model_training =  r"C:\Users\pups1\PycharmProjects\project\Lstmautoencoder_for_malwareapi\finalcsv\Browsefox\2bf54e49fc3e09e889819a47d68cf82c975542d3f0a6a41e6306fdb25d74a0c5_3316.csv"


saved_model = "model_combine_autoencoder_for_3316_Browsefox.h5"
apa = 200
batch = 10


def gothrougheveryfile(dir):  # go through every csv and concat to one csv this part has move to concatallcsv
    i = 0
    filenamelist = list()
    for filename in os.listdir(dir):  # outside each folder

        wholefilepath = dir + "//" + filename
        filenamelist.append(wholefilepath)

    return filenamelist


def data_to_reconstruction_problem(data, timestep):
    df = DataFrame(data)
    list_concat = list()
    for i in range(timestep - 1, -1, -1):
        tempdf = df.shift(i)
        list_concat.append(tempdf)
    data_for_autoencoder = concat(list_concat, axis=1)
    data_for_autoencoder.dropna(inplace=True)
    return data_for_autoencoder




def out_put_core(writting_list, outaddress, filename):
    thefile = open(outaddress + "\\" + filename + ".txt", 'w')
    for item in writting_list:
        thefile.write("%s\n" % item)

def data_preprocess(file_name):
    # read
    dataset = read_csv(file_name, header=None, index_col=None)
    if dataset.shape[0]<16:
        return 0,0

    values = dataset.values

    reframed = data_to_reconstruction_problem(values, n_apis)


    #print("reframed")
    #print(reframed.shape)

    # to 0 - 1
    reframed = reframed.astype('float32')
    scaler = MinMaxScaler(feature_range=(0, 1))
    scaled = scaler.fit_transform(reframed)
    # #print ("deal with: "+file_name+" in folder: "+folder+" shape: "+str(dfscaled.shape))
    dfscaled = DataFrame(scaled)
    valuescaled =dfscaled.values
    print("123123132")
    print(valuescaled.shape)
    return  valuescaled,scaler

'''  
for folder in os.listdir(folder_address):
    if folder.endswith('.py') or folder.endswith('.txt'):
        continue
    if not os.path.exists(preoutaddress+folder):
        os.makedirs(preoutaddress+folder)
    file_address = folder_address + "\\" + folder
    outputaddress = preoutaddress + folder
    for file in os.listdir(file_address):
    '''
############################################################# tab 2

n_apis = 16
n_features = 25


# file_name = file_address + "\\" + file
#for i in range(0,len(filepathlist)):

#train_X, scaler = data_preprocess(data_for_model_training)
train_X, scaler = data_preprocess(data_for_model_training)
# train_X.shape[0] is row number in csv(sample number)
train_X = train_X.reshape((train_X.shape[0], n_apis, n_features))
sample_number = train_X.shape[0]

outputlayer2 = n_apis #16
outputlayer3 = int(n_apis / 2) #8
timesstep4 = int(n_apis / 4) # 4
# model.add(LSTM(34, input_shape=(train_X.shape[1], train_X.shape[2]), return_sequences=False))

# 拉model structure
# model layer

model = Sequential()
model.add(LSTM(n_features, input_shape=(n_apis, train_X.shape[2]), return_sequences=True))  # train_X.shape[2] = 34

model.add(LSTM(outputlayer2, return_sequences=True))

model.add(LSTM(outputlayer3, return_sequences=True))

model.add(LSTM(outputlayer2, return_sequences=True))

model.add(LSTM(n_features, return_sequences=True))
model.compile(loss='mse', optimizer='adam')

# model layer end

# start training
history = model.fit(train_X, train_X, epochs=apa, batch_size=batch, shuffle=False)  # train


model.save(saved_model)



