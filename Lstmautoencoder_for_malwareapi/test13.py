from math import sqrt
from numpy import concatenate
from matplotlib import pyplot
import numpy as np
from pandas import read_csv
from pandas import DataFrame
from pandas import concat
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import mean_squared_error
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import LSTM
from keras.layers.core import RepeatVector
from keras.models import model_from_yaml
import os
from keras.utils import plot_model

from keras.utils.vis_utils import plot_model

from keras.utils.vis_utils import model_to_dot

#可決定的參數
# data_for_model_training 要用哪筆csv去做training
# saved_model 要儲存的model名稱
# apa 要訓練的次數
# batch

# 多階層的lstm的autoencoder
# 用來壓縮hooklog 輸出hooklog hat然後跟原始的hooklog比對
# 取latent做分類用
# 此一檔案test1
# 用來一筆資料建model之後
# 同一個家族的其他筆csv拿來測準確度
# stride is 1 所以資料sampleg數會少15筆
# test3.py用來測試儲存訓練好的model
# 可用這個檔案來產生model 並儲存
# test3.py 為training 的部分
# test6.py 不將 data展開成為每個row16個timestep的格式
# 嘗試重寫成 1個row一個timestep
# 將一個類別裡的每個autoencoder合併嘗試找到一個大的autoencoder來代表這個家族

# test11.py
# 針對清洗過的樣本
# 實驗如果針對單一個家族裡的一個樣本做一個autoencoder還原的值會是多少
# 這個實驗用其中一個樣本做autoencoder其他的用來predict


# sample class bary

data_for_model_training_1 = r"C:\Users\pups1\PycharmProjects\project\Lstmautoencoder_for_malwareapi\finalcsv\Browsefox\1ee514676a8305eebf1b945d0c14e94b17e2b950ee2b2cad054623675a033199_3256.csv"
data_for_model_training_2 = r"C:\Users\pups1\PycharmProjects\project\Lstmautoencoder_for_malwareapi\finalcsv\Browsefox\2bf54e49fc3e09e889819a47d68cf82c975542d3f0a6a41e6306fdb25d74a0c5_3316.csv"
data_for_model_training_3 = r"C:\Users\pups1\PycharmProjects\project\Lstmautoencoder_for_malwareapi\finalcsv\Browsefox\2ccbcc893d4baa3f44afadb97e097a14037e83e0972eed309ce5eb3490f6f78b_3324.csv"
data_for_model_training_4 = r"C:\Users\pups1\PycharmProjects\project\Lstmautoencoder_for_malwareapi\finalcsv\Browsefox\3aa60ef86be648b44c67d4eb61ba978d06b554f82df1c1e65bc039bfb334e657_3292.csv"
data_for_model_training_5 = r"C:\Users\pups1\PycharmProjects\project\Lstmautoencoder_for_malwareapi\finalcsv\Browsefox\2160d853ced0f5d19c35fb0e655e9cdcc9811b7a3259380a8404edeb13a216db_3244.csv"
data_for_model_training_6 = r"C:\Users\pups1\PycharmProjects\project\Lstmautoencoder_for_malwareapi\finalcsv\Browsefox\fff6c2061156acb1af15e00e151fac70_3216.csv"
saved_model = "model_single_autoencoder.h5"
malware_class_dir = r"C:\Users\pups1\PycharmProjects\project\Lstmautoencoder_for_malwareapi\finalcsv\Browsefox"
apa = 1
batch = 10


def gothrougheveryfile(dir):  # go through every csv and concat to one csv this part has move to concatallcsv
    i = 0
    filenamelist = list()
    for filename in os.listdir(dir):  # outside each folder

        wholefilepath = dir + "//" + filename
        filenamelist.append(wholefilepath)

    return filenamelist


def data_to_reconstruction_problem(data, timestep):
    df = DataFrame(data)
    list_concat = list()
    for i in range(timestep - 1, -1, -1):
        tempdf = df.shift(i)
        list_concat.append(tempdf)
    data_for_autoencoder = concat(list_concat, axis=1)
    data_for_autoencoder.dropna(inplace=True)
    return data_for_autoencoder




def out_put_core(writting_list, outaddress, filename):
    thefile = open(outaddress + "\\" + filename + ".txt", 'w')
    for item in writting_list:
        thefile.write("%s\n" % item)

def data_preprocess(file_name):
    # read file
    dataset = read_csv(file_name, header=None, index_col=None)
    #如果csv sample數量少於16則會發生錯誤因此返回
    if dataset.shape[0]<=16:
        return 0,0,0

    values = dataset.values

    reframed = data_to_reconstruction_problem(values, n_apis)

    #return inorder to compare test_x y_hat
    reframedvalues =reframed


    print("reframed")
    print(reframed.shape)

    # to 0 - 1
    reframed = reframed.astype('float32')
    scaler = MinMaxScaler(feature_range=(0, 1))
    scaled = scaler.fit_transform(reframed)
    # print ("deal with: "+file_name+" in folder: "+folder+" shape: "+str(dfscaled.shape))
    dfscaled = DataFrame(scaled)
    valuescaled =dfscaled.values

    return  valuescaled,scaler,reframedvalues


def autoencoderstructureandtraining(data_for_model_training):
    # train_X, scaler = data_preprocess(data_for_model_training)
    train_X, scaler,y = data_preprocess(data_for_model_training)
    # train_X.shape[0] is row number in csv(sample number)
    train_X = train_X.reshape((train_X.shape[0], n_apis, n_features))
    sample_number = train_X.shape[0]

    outputlayer2 = n_apis  # 16
    outputlayer3 = int(n_apis / 2)  # 8
    timesstep4 = int(n_apis / 4)  # 4
    # 拉model structure
    # model layer

    model = Sequential()
    model.add(LSTM(n_features, input_shape=(n_apis, train_X.shape[2]), return_sequences=True))  # train_X.shape[2] = 34

    model.add(LSTM(outputlayer2, return_sequences=True))

    model.add(LSTM(outputlayer3, return_sequences=True))

    model.add(LSTM(outputlayer2, return_sequences=True))

    model.add(LSTM(n_features, return_sequences=True))
    model.compile(loss='mse', optimizer='adam')

    # model layer end

    # start training
    history = model.fit(train_X, train_X, epochs=apa, batch_size=batch, shuffle=False)  # train
    # model.save(saved_model)
    return  model

'''  
for folder in os.listdir(folder_address):
    if folder.endswith('.py') or folder.endswith('.txt'):
        continue
    if not os.path.exists(preoutaddress+folder):
        os.makedirs(preoutaddress+folder)
    file_address = folder_address + "\\" + folder
    outputaddress = preoutaddress + folder
    for file in os.listdir(file_address):
    '''
############################################################# tab 2

n_apis = 16
n_features = 25


model1 =autoencoderstructureandtraining(data_for_model_training_1)
model2 =autoencoderstructureandtraining(data_for_model_training_2)
model3 =autoencoderstructureandtraining(data_for_model_training_3)
model4 =autoencoderstructureandtraining(data_for_model_training_4)
model5 =autoencoderstructureandtraining(data_for_model_training_5)

filepathlist = gothrougheveryfile(malware_class_dir)


#for i in range(0,len(filepathlist)):
for i in range(0, 1):
    print(filepathlist[i])
    filepathlist = gothrougheveryfile(malware_class_dir)

    test_x, scaler,y = data_preprocess(filepathlist[i])

    if test_x is 0:
        continue

    #print(test_x.shape)
    test_x = test_x.reshape(test_x.shape[0], n_apis, n_features)
    # returns a compiled model
    # identical to the previous one
    #model = load_model(saved_model)

    #當我們在train的時候採用的是展開(timestep16)後的樣本，因此預測的自己也(yhat)會是展開後的樣本
    yhat1 = model1.predict(test_x)
    yhat2 = model2.predict(test_x)
    yhat3 = model3.predict(test_x)
    yhat4 = model4.predict(test_x)
    yhat5 = model5.predict(test_x)

    yhat1 = yhat1.reshape(yhat1.shape[0], n_apis * n_features)
    yhat2 = yhat2.reshape(yhat2.shape[0], n_apis * n_features)
    yhat3 = yhat3.reshape(yhat3.shape[0], n_apis * n_features)
    yhat4 = yhat4.reshape(yhat4.shape[0], n_apis * n_features)
    yhat5 = yhat5.reshape(yhat5.shape[0], n_apis * n_features)

yhat1 =yhat1.tolist()
yhat2 =yhat2.tolist()
yhat3 =yhat3.tolist()
yhat4 =yhat4.tolist()
yhat5 =yhat5.tolist()

dense_input = [[10 for k in range(1)] for j in range(len(yhat1))]

for i in range(len(yhat1)):
    dense_input[i]= yhat1[i]+yhat2[i]+yhat3[i]+yhat4[i]+ yhat5[i]

dense_input= np.array(dense_input)

dense_input =dense_input.reshape(dense_input.shape[0],5,400)


for i in range(dense_input.shape[0]):
    if i is 0:
        dense_input = dense_input.tolist()
    dense_input[i] = list(map(list, zip(*dense_input[i])))
   # dense_input[i] = np.transpose(dense_input[i])

dense_input =np.array(dense_input)
print(dense_input.shape)
print(dense_input.shape)
print(dense_input)
samples = dense_input.shape[0]
apismulfeatures= dense_input.shape[1]
models = dense_input[2]
dense_input = dense_input.reshape(samples*apismulfeatures,models)
model = Sequential()
model.add(Dense(1, input_shape=(5,)))

model.compile(loss='mse', optimizer='adam')

# model layer end

# start training
history = model.fit(train_X, train_X, epochs=apa, batch_size=batch, shuffle=False)  # train
#yhat4 =yhat4.tolist()
#yhat5 =yhat5.tolist()


'''
for i in range(0,yhat1.shape[0]) :
    for j in range(0,yhat1.shape[1]):
        #newinput[i,j] =yhat1[i,j,:]
        newinput = np.concatenate((yhat1[i,j],yhat2[i,j], b.T), axis=1)

    print(yhat1.shape)
    print(yhat2.shape)
    print(yhat3.shape)
    print(yhat4.shape)
    print(yhat5.shape)



# file_name = file_address + "\\" + file

# model.add(LSTM(34, input_shape=(train_X.shape[1], train_X.shape[2]), return_sequences=False))


#model.save(saved_model)


yhat = model.predict(test_x)
# 輸出的yhat.shape是 (sample數,apis,n_feature(34)) 其中sample數同原始sample數就是原始csv檔的個數
# 因此要reshape成原本的 shape(transform之後的shape)才能 inverse_transform
yhat = yhat.reshape(yhat.shape[0], n_apis * n_features)
yhat = scaler.inverse_transform(yhat)
'''