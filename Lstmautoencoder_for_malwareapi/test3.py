from math import sqrt
from numpy import concatenate
from matplotlib import pyplot
import numpy as np
from pandas import read_csv
from pandas import DataFrame
from pandas import concat
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import mean_squared_error
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import LSTM
from keras.layers.core import RepeatVector
from keras.models import model_from_yaml
import os
from keras.utils import plot_model

from keras.utils.vis_utils import plot_model

from keras.utils.vis_utils import model_to_dot

#可決定的參數
# data_for_model_training 要用哪筆csv去做training
# saved_model 要儲存的model名稱
# apa 要訓練的次數
# batch

# 多階層的lstm的autoencoder
# 用來壓縮hooklog 輸出hooklog hat然後跟原始的hooklog比對
# 取latent做分類用
# 此一檔案test1
# 用來一筆資料建model之後
# 同一個家族的其他筆csv拿來測準確度
# stride is 1 所以資料sampleg數會少15筆
# test3.py用來測試儲存訓練好的model
# 可用這個檔案來產生model 並儲存
# test3.py 為training 的部分

malware_class_dir = "C:\\Users\\pups1\\PycharmProjects\\project\\Lstmautoencoder_for_malwareapi\\barys"
#data_for_model_training =  r"C:\\Users\pups1\PycharmProjects\project\Lstmautoencoder_for_malwareapi\almanahe\\2c9490efbc41f09429190744bceecbca6b1104117930fca2f25254a2af416d3a_3480.csv"
#data_for_model_training =  r"C:\\Users\pups1\PycharmProjects\project\Lstmautoencoder_for_malwareapi\almanahe\\9becb8a2f6ae59cb066fd11b187a966f2e192665909cea737bb86b43bbd7f6c6_3300.csv"

# sample class bary
data_for_model_training =r"C:\Users\pups1\PycharmProjects\project\Lstmautoencoder_for_malwareapi\barys\8a6492bb182f3666c0b63ff763120d3c23f12889853ea35f534ab1d0dc912c64_3264.csv"
saved_model = "model_apa1000_barys.h5"
apa = 1000
batch = 10


def gothrougheveryfile(dir):  # go through every csv and concat to one csv this part has move to concatallcsv
    i = 0
    filenamelist = list()
    for filename in os.listdir(dir):  # outside each folder

        wholefilepath = dir + "//" + filename
        filenamelist.append(wholefilepath)

    return filenamelist


def data_to_reconstruction_problem(data, timestep):
    df = DataFrame(data)
    list_concat = list()
    for i in range(timestep - 1, -1, -1):
        tempdf = df.shift(i)
        list_concat.append(tempdf)
    data_for_autoencoder = concat(list_concat, axis=1)
    data_for_autoencoder.dropna(inplace=True)
    return data_for_autoencoder


def data_to_reconstruction_problem(data, timestep):
    df = DataFrame(data)
    list_concat = list()
    for i in range(timestep - 1, -1, -1):
        tempdf = df.shift(i)
        list_concat.append(tempdf)
    data_for_autoencoder = concat(list_concat, axis=1)
    data_for_autoencoder.dropna(inplace=True)
    return data_for_autoencoder


def out_put_core(writting_list, outaddress, filename):
    thefile = open(outaddress + "\\" + filename + ".txt", 'w')
    for item in writting_list:
        thefile.write("%s\n" % item)

def data_preprocess(file_name):
    # read
    dataset = read_csv(file_name, header=None, index_col=None)
    if dataset.shape[0]<=16:
        return 0,0

    values = dataset.values

    reframed = data_to_reconstruction_problem(values, n_apis)


    print("reframed")
    print(reframed.shape)

    # to 0 - 1
    reframed = reframed.astype('float32')
    scaler = MinMaxScaler(feature_range=(0, 1))
    scaled = scaler.fit_transform(reframed)
    # print ("deal with: "+file_name+" in folder: "+folder+" shape: "+str(dfscaled.shape))
    dfscaled = DataFrame(scaled)
    valuescaled =dfscaled.values

    return  valuescaled,scaler

'''  
for folder in os.listdir(folder_address):
    if folder.endswith('.py') or folder.endswith('.txt'):
        continue
    if not os.path.exists(preoutaddress+folder):
        os.makedirs(preoutaddress+folder)
    file_address = folder_address + "\\" + folder
    outputaddress = preoutaddress + folder
    for file in os.listdir(file_address):
    '''
############################################################# tab 2

# file_name = file_address + "\\" + file


filepathlist = gothrougheveryfile(malware_class_dir)
n_apis = 16
n_features = 34

train_X, scaler = data_preprocess(data_for_model_training)

# train_X.shape[0] is row number in csv(sample number)
train_X = train_X.reshape((train_X.shape[0], n_apis, n_features))
sample_number = train_X.shape[0]

timesstep16 = n_apis
timesstep8 = int(n_apis / 2)
timesstep4 = int(n_apis / 4)
# model.add(LSTM(34, input_shape=(train_X.shape[1], train_X.shape[2]), return_sequences=False))

# 拉model structure
# model layer
model = Sequential()
model.add(LSTM(n_features, input_shape=(n_apis, train_X.shape[2]), return_sequences=True))

model.add(LSTM(timesstep16, return_sequences=True))

model.add(LSTM(timesstep8, return_sequences=True))

model.add(LSTM(timesstep16, return_sequences=True))

model.add(LSTM(n_features, return_sequences=True))
model.compile(loss='mse', optimizer='adam')
# model layer end

# start training
history = model.fit(train_X, train_X, epochs=apa, batch_size=batch, shuffle=False)  # train
model.save(saved_model)

pyplot.plot(history.history['loss'], label='train')
pyplot.legend()
# pyplot.show()

#lossepoch = "{}.png".format(os.path.basename(filepathlist[i]))
lossepoch = "{}.png".format(data_for_model_training)
pyplot.savefig(lossepoch)
pyplot.gcf().clear()

#以下是test的過程在test4.py重寫
'''
for i in range(0, len(filepathlist)):
    print(filepathlist[i])

    test_x, scaler = data_preprocess(filepathlist[i])

    if test_x is 0:
        continue

    print(test_x.shape)
    test_x = test_x.reshape(test_x.shape[0], n_apis, n_features)
    yhat = model.predict(test_x)
    # 輸出的yhat.shape是 (sample數,apis,n_feature(34)) 其中sample數同原始sample數就是原始csv檔的個數
    # 因此要reshape成原本的 shape(transform之後的shape)才能 inverse_transform
    yhat = yhat.reshape(yhat.shape[0], n_apis * n_features)
    yhat = scaler.inverse_transform(yhat)
    dfyhat = DataFrame(yhat)
    yhat_reconstruction_value = "recons.{}.csv".format(os.path.basename(filepathlist[i]))
    dfyhat.to_csv(yhat_reconstruction_value)


    test_x =0
    scaler =0
'''
