from math import sqrt
from numpy import concatenate
from matplotlib import pyplot
import numpy as np
from pandas import read_csv
from pandas import DataFrame
from pandas import concat
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import mean_squared_error
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import LSTM
from keras.layers.core import RepeatVector
from keras.models import model_from_yaml
import os
from keras.utils import plot_model

from keras.utils.vis_utils import plot_model

from keras.utils.vis_utils import model_to_dot

#可決定的參數
# data_for_model_training 要用哪筆csv去做training
# saved_model 要儲存的model名稱
# apa 要訓練的次數
# batch

# 多階層的lstm的autoencoder
# 用來壓縮hooklog 輸出hooklog hat然後跟原始的hooklog比對
# 取latent做分類用
# 此一檔案test1
# 用來一筆資料建model之後
# 同一個家族的其他筆csv拿來測準確度
# stride is 1 所以資料sampleg數會少15筆
# test3.py用來測試儲存訓練好的model
# 可用這個檔案來產生model 並儲存
# test3.py 為training 的部分
# test6.py 不將 data展開成為每個row16個timestep的格式
# 嘗試重寫成 1個row一個timestep
# 將一個類別裡的每個autoencoder合併嘗試找到一個大的autoencoder來代表這個家族


#malware_class_dir = "C:\\Users\\pups1\\PycharmProjects\\project\\Lstmautoencoder_for_malwareapi\\barys"
malware_class_dir = r"C:\Users\pups1\PycharmProjects\project\Lstmautoencoder_for_malwareapi\finalcsv\Browsefox"
#data_for_model_training =  r"C:\\Users\pups1\PycharmProjects\project\Lstmautoencoder_for_malwareapi\almanahe\\2c9490efbc41f09429190744bceecbca6b1104117930fca2f25254a2af416d3a_3480.csv"
#data_for_model_training =  r"C:\\Users\pups1\PycharmProjects\project\Lstmautoencoder_for_malwareapi\almanahe\\9becb8a2f6ae59cb066fd11b187a966f2e192665909cea737bb86b43bbd7f6c6_3300.csv"

# sample class bary
#data_for_model_training =r"C:\Users\pups1\PycharmProjects\project\Lstmautoencoder_for_malwareapi\barys\8a6492bb182f3666c0b63ff763120d3c23f12889853ea35f534ab1d0dc912c64_3264.csv"
data_for_model_training =r"C:\Users\pups1\PycharmProjects\project\Lstmautoencoder_for_malwareapi\finalcsv\Bdmj\0e5d047639d4fa1284e222c534bb3998aebb0654d5464f39180c7ec3fdf3baff_3276.csv"
saved_model = "model_combine_autoencoder_for_Browsefox.h5"
apa = 200
batch = 10


def gothrougheveryfile(dir):  # go through every csv and concat to one csv this part has move to concatallcsv
    i = 0
    filenamelist = list()
    for filename in os.listdir(dir):  # outside each folder

        wholefilepath = dir + "//" + filename
        filenamelist.append(wholefilepath)

    return filenamelist


def data_to_reconstruction_problem(data, timestep):
    df = DataFrame(data)
    list_concat = list()
    for i in range(timestep - 1, -1, -1):
        tempdf = df.shift(i)
        list_concat.append(tempdf)
    data_for_autoencoder = concat(list_concat, axis=1)
    data_for_autoencoder.dropna(inplace=True)
    return data_for_autoencoder




def out_put_core(writting_list, outaddress, filename):
    thefile = open(outaddress + "\\" + filename + ".txt", 'w')
    for item in writting_list:
        thefile.write("%s\n" % item)

def data_preprocess(file_name):
    # read
    dataset = read_csv(file_name, header=None, index_col=None)
    if dataset.shape[0]<16:
        return 0,0

    values = dataset.values

    reframed = data_to_reconstruction_problem(values, n_apis)


    #print("reframed")
    #print(reframed.shape)

    # to 0 - 1
    reframed = reframed.astype('float32')
    scaler = MinMaxScaler(feature_range=(0, 1))
    scaled = scaler.fit_transform(reframed)
    # #print ("deal with: "+file_name+" in folder: "+folder+" shape: "+str(dfscaled.shape))
    dfscaled = DataFrame(scaled)
    valuescaled =dfscaled.values
    print("123123132")
    print(valuescaled.shape)
    return  valuescaled,scaler

'''  
for folder in os.listdir(folder_address):
    if folder.endswith('.py') or folder.endswith('.txt'):
        continue
    if not os.path.exists(preoutaddress+folder):
        os.makedirs(preoutaddress+folder)
    file_address = folder_address + "\\" + folder
    outputaddress = preoutaddress + folder
    for file in os.listdir(file_address):
    '''
############################################################# tab 2

n_apis = 16
n_features = 25
filepathlist = gothrougheveryfile(malware_class_dir)
#error_list = list()
W_Hidden1_list = list()
W_Hidden2_list = list()
W_Hidden3_list = list()
W_Hidden4_list = list()
W_Hidden5_list = list()


# file_name = file_address + "\\" + file
for i in range(0,len(filepathlist)):

    print(filepathlist[i])

    #train_X, scaler = data_preprocess(data_for_model_training)
    train_X, scaler = data_preprocess(filepathlist[i])
    # train_X.shape[0] is row number in csv(sample number)
    train_X = train_X.reshape((train_X.shape[0], n_apis, n_features))
    sample_number = train_X.shape[0]

    outputlayer2 = n_apis #16
    outputlayer3 = int(n_apis / 2) #8
    timesstep4 = int(n_apis / 4) # 4
    # model.add(LSTM(34, input_shape=(train_X.shape[1], train_X.shape[2]), return_sequences=False))

    # 拉model structure
    # model layer

    model = Sequential()
    model.add(LSTM(n_features, input_shape=(n_apis, train_X.shape[2]), return_sequences=True))  # train_X.shape[2] = 34

    model.add(LSTM(outputlayer2, return_sequences=True))

    model.add(LSTM(outputlayer3, return_sequences=True))

    model.add(LSTM(outputlayer2, return_sequences=True))

    model.add(LSTM(n_features, return_sequences=True))
    model.compile(loss='mse', optimizer='adam')

    # model layer end

    # start training
    history = model.fit(train_X, train_X, epochs=apa, batch_size=batch, shuffle=False)  # train
    #model.save(saved_model)



    #getweight[0] 是weights [1] 是 bias
    W_Hidden1 = model.layers[0].get_weights()
    W_Hidden2 = model.layers[1].get_weights()
    W_Hidden3 = model.layers[2].get_weights()
    W_Hidden4 = model.layers[3].get_weights()
    W_Hidden5 = model.layers[4].get_weights()

    print("pppppppppppppppppppppppppppppppppppp")
    print(len(W_Hidden3))
    print((W_Hidden3[0].shape))
    print((W_Hidden3[1].shape))
    print((W_Hidden3[2].shape))
    print("pppppppppppppppppppppppppppppppppppp")
    print(i)
    #print(type(W_Hidden1))
    #print(len(W_Hidden1))
    #print(W_Hidden1[0].shape)
    #print(W_Hidden1[1].shape)
    #print(W_Hidden1[2].shape)
    #print("dsddsdsdsdsdds")
    W_Hidden1_list.append(W_Hidden1)
    W_Hidden2_list.append(W_Hidden2)
    W_Hidden3_list.append(W_Hidden3)
    W_Hidden4_list.append(W_Hidden4)
    W_Hidden5_list.append(W_Hidden5)
    #a =np.array(W_Hidden1_list,axis=0)
    #a = np.concatenate(W_Hidden1_list,axis =1)
    ##print(W_Hidden1.shape)
    ##print(W_Hidden2.shape)
    #(W_Hidden4.shape)


    #print("hellohello")







#print(len(W_Hidden1_list))

weight_W_Hidden1_list = []
bias_W_Hidden1_list   = []
dontnow_W_Hidden1_list = []

weight_W_Hidden2_list = []
bias_W_Hidden2_list   = []
dontnow_W_Hidden2_list = []

weight_W_Hidden3_list = []
bias_W_Hidden3_list   = []
dontnow_W_Hidden3_list = []

weight_W_Hidden4_list = []
bias_W_Hidden4_list   = []
dontnow_W_Hidden4_list = []

weight_W_Hidden5_list = []
bias_W_Hidden5_list   = []
dontnow_W_Hidden5_list = []

#  split weight bias dontnow to diff list
#  len of W_Hidden1_list means the number of samples
for i in range(len(W_Hidden1_list)):
    weight_W_Hidden1_list.append(W_Hidden1_list[i][0])
    bias_W_Hidden1_list.append(W_Hidden1_list[i][1])
    dontnow_W_Hidden1_list.append(W_Hidden1_list[i][2])

    weight_W_Hidden2_list.append(W_Hidden2_list[i][0])
    bias_W_Hidden2_list.append(W_Hidden2_list[i][1])
    dontnow_W_Hidden2_list.append(W_Hidden2_list[i][2])

    weight_W_Hidden3_list.append(W_Hidden3_list[i][0])
    bias_W_Hidden3_list.append(W_Hidden3_list[i][1])
    dontnow_W_Hidden3_list.append(W_Hidden3_list[i][2])

    weight_W_Hidden4_list.append(W_Hidden4_list[i][0])
    bias_W_Hidden4_list.append(W_Hidden4_list[i][1])
    dontnow_W_Hidden4_list.append(W_Hidden4_list[i][2])

    weight_W_Hidden5_list.append(W_Hidden5_list[i][0])
    bias_W_Hidden5_list.append(W_Hidden5_list[i][1])
    dontnow_W_Hidden5_list.append(W_Hidden5_list[i][2])

hidden1_combine_weight =np.mean(weight_W_Hidden1_list,axis =0)
hidden1_combine_bias =np.mean(bias_W_Hidden1_list,axis =0)
hidden1_combine_dontnow = np.mean(dontnow_W_Hidden1_list,axis =0)

hidden2_combine_weight = np.mean(weight_W_Hidden2_list,axis =0)
hidden2_combine_bias = np.mean(bias_W_Hidden2_list,axis =0)
hidden2_combine_dontnow = np.mean(dontnow_W_Hidden2_list,axis =0)

hidden3_combine_weight = np.mean(weight_W_Hidden3_list,axis =0)
hidden3_combine_bias = np.mean(bias_W_Hidden3_list,axis =0)
hidden3_combine_dontnow = np.mean(dontnow_W_Hidden3_list,axis =0)

hidden4_combine_weight = np.mean(weight_W_Hidden4_list,axis =0)
hidden4_combine_bias = np.mean(bias_W_Hidden4_list,axis =0)
hidden4_combine_dontnow = np.mean(dontnow_W_Hidden4_list,axis =0)

hidden5_combine_weight = np.mean(weight_W_Hidden5_list,axis =0)
hidden5_combine_bias = np.mean(bias_W_Hidden5_list,axis =0)
hidden5_combine_dontnow = np.mean(dontnow_W_Hidden5_list,axis =0)
#print("----------")
#print(a.shape)
#print(b.shape)
#print(c.shape)
hidden1_combine = []
hidden2_combine = []
hidden3_combine = []
hidden4_combine = []
hidden5_combine = []

hidden1_combine.append(hidden1_combine_weight)
hidden1_combine.append(hidden1_combine_bias)
hidden1_combine.append(hidden1_combine_dontnow)

hidden2_combine.append(hidden2_combine_weight)
hidden2_combine.append(hidden2_combine_bias)
hidden2_combine.append(hidden2_combine_dontnow)

hidden3_combine.append(hidden3_combine_weight)
hidden3_combine.append(hidden3_combine_bias)
hidden3_combine.append(hidden3_combine_dontnow)

hidden4_combine.append(hidden4_combine_weight)
hidden4_combine.append(hidden4_combine_bias)
hidden4_combine.append(hidden4_combine_dontnow)

hidden5_combine.append(hidden5_combine_weight)
hidden5_combine.append(hidden5_combine_bias)
hidden5_combine.append(hidden5_combine_dontnow)


#print("ggggg")
#print (len(k))




model_combine= Sequential()




#model1.layers[0].set_weights(k)

#print ("dgg")

# hidden layer 1
model_combine.add(LSTM(n_features, input_shape=(n_apis, train_X.shape[2]), return_sequences=True))  # train_X.shape[2] = 34

# hidden layer 2
model_combine.add(LSTM(outputlayer2, return_sequences=True))

# hidden layer 3
model_combine.add(LSTM(outputlayer3, return_sequences=True))

# hidden layer 4
model_combine.add(LSTM(outputlayer2, return_sequences=True))

# hidden layer 5
model_combine.add(LSTM(n_features, return_sequences=True))

model_combine.layers[0].set_weights(hidden1_combine)
model_combine.layers[1].set_weights(hidden2_combine)

model_combine.layers[2].set_weights(hidden3_combine)
model_combine.layers[3].set_weights(hidden4_combine)
model_combine.layers[4].set_weights(hidden5_combine)


#load_weights
model_combine.compile(loss='mse', optimizer='adam')
print ("dgg")

model_combine.save(saved_model)


